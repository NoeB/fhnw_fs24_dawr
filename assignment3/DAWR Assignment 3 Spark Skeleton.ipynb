{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "38924a7b-7cef-4ea1-a4d6-2c43be890d10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Part 1 - Parsing Hikes\n",
    "\n",
    "In the first part of the assignment, you need to extract the relevant attributes from the web pages scraped from hikr.org. Extend the `parse` function so that it extracts all the attributes you need to create the ranking. You may define your own helper functions and extend the `parse` function as necessary. Just keep in mind that the arguments/result types should not be changed to enable you to use the function in the second part of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "716140a3-fd70-489b-af13-73e2edd565bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.selector import Selector\n",
    "from datetime import datetime, date, timedelta\n",
    "import locale\n",
    "import lib.parser_functions as pf\n",
    "from typing import List\n",
    "locale.setlocale(locale.LC_TIME, 'de_DE')\n",
    "from dataclasses import dataclass\n",
    "from lib.parser_functions import HikingDifficulty, ClimbingDifficulty, MountainBikeDifficulty, SnowshoeTourDifficulty, Waypoint, Peak, TourPartner\n",
    "\n",
    "@dataclass\n",
    "class ParsedTour:\n",
    "    name: str\n",
    "    id: str\n",
    "    author_public_name: str\n",
    "    author_internal_name: str\n",
    "    author_id: str\n",
    "    publishing_date_str: str\n",
    "    publishing_date: datetime\n",
    "    photo_count: int\n",
    "    peaks: List[Peak]\n",
    "    regions: dict\n",
    "    tour_date: date\n",
    "    waypoints: List[Waypoint]\n",
    "    hiking_difficulty: HikingDifficulty\n",
    "    ascent: int\n",
    "    descent: int\n",
    "    duration: timedelta\n",
    "    climbing_difficulty: ClimbingDifficulty\n",
    "    hightour_difficulty: str\n",
    "    mountain_bike_difficulty: MountainBikeDifficulty\n",
    "    via_ferrata_difficulty: str\n",
    "    ski_difficulty: str\n",
    "    snowshoe_difficulty: SnowshoeTourDifficulty\n",
    "    tour_partner: List[TourPartner]\n",
    "    page_views: int\n",
    "# Parses a hikr.org tour and extracts all the attributes we are interested in.\n",
    "# Parameters:\n",
    "#   tour: HTML Content of the hikr.org tour.\n",
    "# Result:\n",
    "#   A dictionary containing the extracted attributes for this tour.\n",
    "def parse(tour) -> ParsedTour:\n",
    "    # id is the filename, text is the file content\n",
    "    [id, text] = tour\n",
    "    #id:  ./data/raw/200posts/post24001.html\n",
    "    tour_id = id.split('/')[-1].split('.')[0].replace('post', '')\n",
    "    # Parse it using scrapy\n",
    "    document = Selector(text=text)\n",
    "    # Do some extraction\n",
    "\n",
    "    # TODO: Extract more attributes and add them to the result dictionary!\n",
    "    publishing_date_str = document.css('div.author::text').re_first(r'\\d{1,2}\\. \\w+ \\d{4} um \\d{2}:\\d{2}')\n",
    "    publishing_date = datetime.strptime(publishing_date_str, '%d. %B %Y um %H:%M') if publishing_date_str else None\n",
    "    author_id = document.css('img[id^=\"anchor_author_\"]::attr(id)').re_first(r'anchor_author_(\\d+)')\n",
    "    raw_object = {\n",
    "\n",
    "    }\n",
    "\n",
    "    for attribute in pf.columns:\n",
    "        raw_content = document.css(f'td.fiche_rando_b:contains(\"{attribute}\") + td.fiche_rando::text').get()\n",
    "        if raw_content:\n",
    "            if raw_content.strip() == '':\n",
    "                raw_object[f'{attribute}_raw'] = document.css(f'td.fiche_rando_b:contains(\"{attribute}\") + td.fiche_rando').get()\n",
    "            else:\n",
    "                raw_object[f'{attribute}_raw'] = raw_content.strip()\n",
    "        else:\n",
    "            raw_object[f'{attribute}_raw'] = None\n",
    "\n",
    "    result = {\n",
    "        'name': document.css('h1.title::text').get(),\n",
    "        'id': tour_id,\n",
    "        'author_public_name': document.css('div.author a.standard::text').get(),\n",
    "        'author_internal_name': document.css('img[id^=\"anchor_author_\"]::attr(onmouseover)').re_first(r'\"https://www.hikr.org/\",\"\\d+\",\"(.*?)\",\"'),\n",
    "        'author_id': author_id,\n",
    "        'publishing_date_str': publishing_date_str,\n",
    "        'publishing_date': publishing_date,\n",
    "        'photo_count': pf.count_photos(text),\n",
    "        'peaks': pf.parse_peak_map(text),\n",
    "        'regions': pf.parse_region(raw_object['Region:_raw']),\n",
    "        'tour_date': pf.parse_tour_date(raw_object['Tour Datum:_raw']),\n",
    "        'waypoints': pf.parse_waypoints(raw_object['Wegpunkte:_raw']),\n",
    "        'hiking_difficulty': pf.parse_hiking_difficulty(raw_object['Wandern Schwierigkeit:_raw']),\n",
    "        'ascent': pf.parse_ascent(raw_object['Aufstieg:_raw']),\n",
    "        'descent': pf.parse_descent(raw_object['Abstieg:_raw']),\n",
    "        'duration': pf.parse_duration(raw_object['Zeitbedarf:_raw']),\n",
    "        'climbing_difficulty': pf.parse_climbing_difficulty(raw_object['Klettern Schwierigkeit:_raw']),\n",
    "        'hightour_difficulty': pf.parse_high_tour_difficulty(raw_object['Hochtouren Schwierigkeit:_raw']),\n",
    "        'mountain_bike_difficulty': pf.parse_mountainbike_difficulty(raw_object['Mountainbike Schwierigkeit:_raw']),\n",
    "        'via_ferrata_difficulty': pf.parse_via_ferrata_difficulty(raw_object['Klettersteig Schwierigkeit:_raw']),\n",
    "        'ski_difficulty': pf.parse_ski_difficulty(raw_object['Ski Schwierigkeit:_raw']),\n",
    "        'snowshoe_difficulty': pf.parse_snowshoe_tour_difficulty(raw_object['Schneeshuhtouren Schwierigkeit:_raw']),\n",
    "        'tour_partner': pf.parse_tour_partners(text),\n",
    "        'page_views': pf.parse_page_views(text)\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "All the helper Function can be found in `/lib/parser_functions.py`\n",
    "\n",
    "And the Jupyternotebook on how I extracted them: `preproccesing_notebook.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9768e05e-f8e8-4dce-9979-e734f897cb6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'monte S. Primo m.1685 (CO) ', 'id': '24001', 'author_public_name': 'Alberto', 'author_internal_name': 'alberto', 'author_id': '2823', 'publishing_date_str': '11. Juni 2010 um 09:25', 'publishing_date': datetime.datetime(2010, 6, 11, 9, 25), 'photo_count': 2, 'peaks': [], 'regions': {'region_0_content': 'Welt', 'country': 'Italien', 'region_2_content': 'Lombardei'}, 'tour_date': datetime.date(2010, 6, 5), 'waypoints': None, 'hiking_difficulty': HikingDifficulty(hiking_difficulty='T2', hiking_difficulty_description='Bergwandern'), 'ascent': 600, 'descent': 600, 'duration': datetime.timedelta(days=1), 'climbing_difficulty': None, 'hightour_difficulty': None, 'mountain_bike_difficulty': None, 'via_ferrata_difficulty': None, 'ski_difficulty': None, 'snowshoe_difficulty': None, 'tour_partner': [{'name': 'Alberto', 'user_id': 'alberto'}], 'page_views': 109}\n"
     ]
    }
   ],
   "source": [
    "# Extract the 200posts.zip file in the same folder where this jupyter notebook is located.\n",
    "# Then you can run the parse function on an example tour:\n",
    "with open('./data/raw/200posts/post24001.html') as f:\n",
    "    content = f.read()\n",
    "    r = parse([f.name, content])\n",
    "    print(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post24199.html\n",
      "post24064.html\n",
      "post24208.html\n",
      "post24025.html\n",
      "post24160.html\n",
      "post24249.html\n",
      "post24320.html\n",
      "post24273.html\n",
      "post24048.html\n",
      "post24224.html\n",
      "post24195.html\n",
      "post24204.html\n",
      "post24212.html\n",
      "post24091.html\n",
      "post24183.html\n",
      "post24117.html\n",
      "post24140.html\n",
      "post24005.html\n",
      "post24156.html\n",
      "post24013.html\n",
      "post24228.html\n",
      "post24101.html\n",
      "post24044.html\n",
      "post24157.html\n",
      "post24012.html\n",
      "post152050.html\n",
      "post24141.html\n",
      "post24004.html\n",
      "post24268.html\n",
      "post24053.html\n",
      "post24182.html\n",
      "post24213.html\n",
      "post24086.html\n",
      "post24194.html\n",
      "post24317.html\n",
      "post24225.html\n",
      "post24049.html\n",
      "post24321.html\n",
      "post24008.html\n",
      "post24233.html\n",
      "post24024.html\n",
      "post24161.html\n",
      "post24136.html\n",
      "post24209.html\n",
      "post152027.html\n",
      "post24065.html\n",
      "post24120.html\n",
      "post24032.html\n",
      "post24198.html\n",
      "post24177.html\n",
      "post24193.html\n",
      "post24202.html\n",
      "post24081.html\n",
      "post24214.html\n",
      "post24078.html\n",
      "post24243.html\n",
      "post24185.html\n",
      "post152016.html\n",
      "post24111.html\n",
      "post24054.html\n",
      "post24146.html\n",
      "post24003.html\n",
      "post24296.html\n",
      "post24150.html\n",
      "post24015.html\n",
      "post24042.html\n",
      "post24170.html\n",
      "post24127.html\n",
      "post152020.html\n",
      "post24131.html\n",
      "post152036.html\n",
      "post24189.html\n",
      "post24166.html\n",
      "post24058.html\n",
      "post24234.html\n",
      "post24019.html\n",
      "post24275.html\n",
      "post24274.html\n",
      "post24262.html\n",
      "post24235.html\n",
      "post24188.html\n",
      "post24167.html\n",
      "post24219.html\n",
      "post24075.html\n",
      "post24130.html\n",
      "post152021.html\n",
      "post24126.html\n",
      "post24034.html\n",
      "post24171.html\n",
      "post24043.html\n",
      "post152001.html\n",
      "post24151.html\n",
      "post24147.html\n",
      "post24002.html\n",
      "post24281.html\n",
      "post24055.html\n",
      "post24239.html\n",
      "post24184.html\n",
      "post24307.html\n",
      "post24079.html\n",
      "post24215.html\n",
      "post24080.html\n",
      "post24203.html\n",
      "post24192.html\n",
      "post24038.html\n",
      "post24254.html\n",
      "post24311.html\n",
      "post152002.html\n",
      "post24152.html\n",
      "post24001.html\n",
      "post24144.html\n",
      "post24282.html\n",
      "post24056.html\n",
      "post24168.html\n",
      "post24187.html\n",
      "post24304.html\n",
      "post24129.html\n",
      "post24083.html\n",
      "post24200.html\n",
      "post24191.html\n",
      "post24109.html\n",
      "post24261.html\n",
      "post24148.html\n",
      "post152018.html\n",
      "post152063.html\n",
      "post24164.html\n",
      "post24021.html\n",
      "post24133.html\n",
      "post24076.html\n",
      "post24125.html\n",
      "post24172.html\n",
      "post24132.html\n",
      "post24165.html\n",
      "post24221.html\n",
      "post24256.html\n",
      "post24190.html\n",
      "post24201.html\n",
      "post24082.html\n",
      "post24169.html\n",
      "post24186.html\n",
      "post24057.html\n",
      "post24283.html\n",
      "post24145.html\n",
      "post24016.html\n",
      "post24153.html\n",
      "post24041.html\n",
      "post24226.html\n",
      "post24158.html\n",
      "post24322.html\n",
      "post24119.html\n",
      "post24162.html\n",
      "post24027.html\n",
      "post24135.html\n",
      "post152024.html\n",
      "post24089.html\n",
      "post24066.html\n",
      "post24174.html\n",
      "post24011.html\n",
      "post24154.html\n",
      "post24292.html\n",
      "post24007.html\n",
      "post24142.html\n",
      "post24284.html\n",
      "post24050.html\n",
      "post24181.html\n",
      "post24247.html\n",
      "post152069.html\n",
      "post24093.html\n",
      "post24139.html\n",
      "post24210.html\n",
      "post24085.html\n",
      "post24206.html\n",
      "post24178.html\n",
      "post24197.html\n",
      "post24314.html\n",
      "post24315.html\n",
      "post24196.html\n",
      "post24084.html\n",
      "post24211.html\n",
      "post24092.html\n",
      "post24138.html\n",
      "post24180.html\n",
      "post152013.html\n",
      "post24051.html\n",
      "post24006.html\n",
      "post24143.html\n",
      "post24010.html\n",
      "post24047.html\n",
      "post24175.html\n",
      "post24030.html\n",
      "post24319.html\n",
      "post24088.html\n",
      "post24067.html\n",
      "post24134.html\n",
      "post152033.html\n",
      "post24163.html\n",
      "post24026.html\n",
      "post24323.html\n",
      "post24159.html\n",
      "post24227.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "files_dir = './data/raw/200posts'\n",
    "for filename in os.listdir(files_dir):\n",
    "    if filename.endswith('.html'):\n",
    "        print(filename)\n",
    "        file_path = os.path.join(files_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            r = parse([f.name, content])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "72a2c7d6-16ce-4c96-8841-41ebfd923fd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Part 2 - Parallelization & Aggregation (Spark)\n",
    "\n",
    "NOTE: It is highly recommended to wait with this part until after the Spark lecture!\n",
    "\n",
    "NOTE: This part only works on databricks!\n",
    "\n",
    "To add a library such as scrapy, perform the following steps:\n",
    "\n",
    "- Go to the \"Clusters\" panel on the left\n",
    "- Select your cluster\n",
    "- Go to the \"Libraries\" tab\n",
    "- Click \"Install New\"\n",
    "- Choose \"PyPI\" as library source\n",
    "- Type the name of the library, \"scrapy\", into the package field\n",
    "- Click \"Install\"\n",
    "- Wait until the installation has finished\n",
    "\n",
    "You can now use the newly installed library in your code.\n",
    "\n",
    "Note: In the community edition, databricks terminates your cluster after 2 hours of inactivity. If you re-create the cluster, you will have to perform these steps again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "76eabc05-2c4a-47da-92b5-169e279fa5b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() \n",
    "\n",
    "\n",
    "\n",
    "    #.config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    #    .config(\"spark.sql.files.maxPartitionBytes\", 1024 * 1024 * 4)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"S3Example\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"15\") \\\n",
    "    .config(\"spark.cores.max\", \"15\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 100) \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.default.parallelism\", 100) \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.files.openCostInBytes\", \"1073741824\" ) \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UnlockExperimentalVMOptions -XX:+UseJVMCICompiler\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UnlockExperimentalVMOptions -XX:+UseJVMCICompiler\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 1024 * 1024 * 4)\n",
    "spark.conf.set(\"spark.sql.files.openCostInBytes\", \"1073741824\")  \n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", os.getenv('S3_KEY_ID'))\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", os.getenv('S3_SECRET_KEY'))\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.getenv('S3_KEY_ID'))\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", os.getenv('S3_SECRET_KEY'))\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "\n",
    "# Contains the whole hikr dataset.\n",
    "# The full dataset contains 113710 tours and has a size of around 6 GB.\n",
    "# There are 46854 posts starting with \"post1*\". Use this dataset for your final results if possible. Execution is likely to take around 30~45 minutes.\n",
    "# There are 8176 posts starting with \"post10*\", which is a nicer size for smaller experiments.\n",
    "# If you want to further shrink the dataset size for testing, you can add another zero to the pattern (post100*.html).\n",
    "#tours = sc.wholeTextFiles(\"s3a://dawr-hikr/post10000*.html\")\n",
    "#tours = sc.wholeTextFiles(\"s3a://dawr-hikr/post10*.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI running on http://YOURIPADDRESS:4040\n"
     ]
    }
   ],
   "source": [
    "# Print spark ui link\n",
    "print('Spark UI running on http://YOURIPADDRESS:' + spark.sparkContext.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f508f8d5-64bb-4744-8941-067ad0db8c16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:======>                                            (1016 + 10) / 8176]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, col, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, TimestampType, ArrayType\n",
    "\n",
    "\n",
    "tours_df = spark.read.text(\"s3a://dawr-hikr/post10*.html\", wholetext=True).withColumn(\"file_path\", input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_udf = udf(lambda content, file_path: parse([file_path, content]), returnType=StructType([\n",
    "    StructField(\"name\", StringType(), nullable=True),\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"author_public_name\", StringType()),\n",
    "    StructField(\"author_internal_name\", StringType()),\n",
    "    StructField(\"author_id\", StringType()),\n",
    "    StructField(\"publishing_date_str\", StringType()),\n",
    "    StructField(\"publishing_date\", TimestampType()),\n",
    "    StructField(\"photo_count\", IntegerType()),\n",
    "    StructField(\"peaks\", ArrayType(StructType([\n",
    "        StructField(\"latitude\", FloatType()),\n",
    "        StructField(\"longitude\", FloatType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"height\", IntegerType()),\n",
    "        StructField(\"id\", IntegerType())\n",
    "    ]))),\n",
    "    StructField(\"regions\", StructType([\n",
    "        StructField(\"region_0_content\", StringType()),\n",
    "        StructField(\"country\", StringType()),\n",
    "        StructField(\"region_2_content\", StringType()),\n",
    "        StructField(\"region_3_content\", StringType()),\n",
    "        StructField(\"region_4_content\", StringType())\n",
    "    ])),\n",
    "    StructField(\"tour_date\", DateType()),\n",
    "    StructField(\"waypoints\", ArrayType(StructType([\n",
    "        StructField(\"image\", StringType()),\n",
    "        StructField(\"name_raw\", StringType()),\n",
    "        StructField(\"type\", StringType()),\n",
    "        StructField(\"waypoint_url\", StringType()),\n",
    "        StructField(\"height\", IntegerType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"peak_id\", StringType())\n",
    "    ]))),\n",
    "    StructField(\"hiking_difficulty\", StructType([\n",
    "        StructField(\"hiking_difficulty\", StringType()),\n",
    "        StructField(\"hiking_difficulty_description\", StringType())\n",
    "    ])),\n",
    "    StructField(\"ascent\", IntegerType()),\n",
    "    StructField(\"descent\", IntegerType()),\n",
    "    StructField(\"duration\", StringType()),  # Spark SQL doesn't have a built-in type for timedelta\n",
    "    StructField(\"climbing_difficulty\", StructType([\n",
    "        StructField(\"climbing_difficulty\", StringType()),\n",
    "        StructField(\"climbing_difficulty_description\", StringType())\n",
    "    ])),\n",
    "    StructField(\"hightour_difficulty\", StringType()),\n",
    "    StructField(\"mountain_bike_difficulty\", StructType([\n",
    "        StructField(\"mountainbike_difficulty\", StringType()),\n",
    "        StructField(\"mountainbike_difficulty_description\", StringType())\n",
    "    ])),\n",
    "    StructField(\"via_ferrata_difficulty\", StringType()),\n",
    "    StructField(\"ski_difficulty\", StringType()),\n",
    "    StructField(\"snowshoe_difficulty\", StructType([\n",
    "        StructField(\"snowshoe_tour_difficulty\", StringType()),\n",
    "        StructField(\"snowshoe_tour_difficulty_description\", StringType())\n",
    "    ])),\n",
    "        StructField(\"tour_partner\", ArrayType(StructType([\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"user_id\", StringType()),\n",
    "    ]))),\n",
    "    StructField(\"page_views\", IntegerType()),\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+---------+--------------------+-------------------+-----------+--------------------+--------------------+----------+--------------------+--------------------+------+-------+--------------------+-------------------+-------------------+------------------------+----------------------+--------------+--------------------+--------------------+----------+\n",
      "|                name|    id|  author_public_name|author_internal_name|author_id| publishing_date_str|    publishing_date|photo_count|               peaks|             regions| tour_date|           waypoints|   hiking_difficulty|ascent|descent|            duration|climbing_difficulty|hightour_difficulty|mountain_bike_difficulty|via_ferrata_difficulty|ski_difficulty| snowshoe_difficulty|        tour_partner|page_views|\n",
      "+--------------------+------+--------------------+--------------------+---------+--------------------+-------------------+-----------+--------------------+--------------------+----------+--------------------+--------------------+------+-------+--------------------+-------------------+-------------------+------------------------+----------------------+--------------+--------------------+--------------------+----------+\n",
      "|Bänke und ihre Au...|108442|          Vielhygler|          vielhygler|    20481|8. November 2016 ...|2016-11-08 23:36:00|        287|[{47.513954, 11.5...|{Welt, Überall, N...|2018-01-20|[{https://s.hikr....|                NULL|  NULL|   NULL|                NULL|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{Winterbaer, Win...|      3614|\n",
      "|  Wandern mit Katzen|100123|                mong|                mong|    12588|2. Januar 2016 um...|2016-01-02 23:32:00|        229|[{46.262966, 9.00...|{Welt, Schweiz, T...|2011-10-18|[{https://s.hikr....| {T4-, Alpinwandern}|   685|    658|Timedelta: 6 days...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|      [{mong, mong}]|      3190|\n",
      "|E5 und mehr - Von...|103600|        Nik Brückner|          Nikbrueckn|    13125|8. Februar 2016 u...|2016-02-08 18:54:00|        139|[{47.503807, 9.73...|{Welt, Terra Inco...|2009-08-07|[{https://s.hikr....| {T4+, Alpinwandern}| 18300|  18450|Timedelta: 17 day...|               NULL|                  L|                    NULL|                     L|          NULL|                NULL|[{Nik Brückner, N...|      3410|\n",
      "|Rifugio Alpe Arbe...|106073|giorgio59m (Girov...|          giorgio59m|     3344|31. März 2016 um ...|2016-03-31 09:07:00|         81|[{46.42771, 9.213...|{Welt, Schweiz, G...|2016-03-26|[{https://s.hikr....|                NULL|   970|   NULL|Timedelta: 5 days...|               NULL|               NULL|                    NULL|                  NULL|          NULL|{WT2, Schneeschuh...|[{giorgio59m (Gir...|      1088|\n",
      "|Garmisch - Vaduz:...|103599|        Nik Brückner|          Nikbrueckn|    13125|23. Januar 2016 u...|2016-01-23 12:09:00|        145|[{47.468254, 11.0...|{Welt, Liechtenst...|2011-08-18|[{https://s.hikr....|  {T4, Alpinwandern}| 19580|  19883|Timedelta: 17 day...|               NULL|                  L|                    NULL|                    WS|          NULL|                NULL|[{Nik Brückner, N...|      1531|\n",
      "|Rund um die Stadt...|106708|          laurentbor|          laurentbor|    23322|27. April 2016 um...|2016-04-27 11:33:00|        255|[{47.40749, 8.587...|{Welt, Schweiz, Z...|2013-05-01|[{https://s.hikr....|                NULL|  1159|   1159|Timedelta: 8 days...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{laurentbor, lau...|      1153|\n",
      "|       Upper Mustang|107904|            amphibol|               reeff|     7930|1. August 2016 um...|2016-08-01 20:42:00|        121|[{28.781376, 83.7...|{Welt, Nepal, Kal...|2016-05-10|[{https://s.hikr....|{T3+, anspruchsvo...|  5050|   4840|Timedelta: 12 day...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{amphibol, reeff...|      1085|\n",
      "|Se \"Angelino\" è G...|106488|               Menek|               Menek|    17406|14. April 2016 um...|2016-04-14 19:20:00|         47|[{45.932934, 8.98...|{Welt, Schweiz, T...|2016-04-12|[{https://s.hikr....|{T3, anspruchsvol...|  1360|   1360|                NULL|               NULL|               NULL|                    NULL|                    WS|          NULL|                NULL|[{gbal, gbal}, {M...|      1117|\n",
      "|CORVEGIA - SASSO ...|108828|        Angelo & Ele|          angeloeleo|    21082|29. Juni 2016 um ...|2016-06-29 19:26:00|         80|[{46.19195, 9.363...|{Welt, Italien, L...|2016-06-28|[{https://s.hikr....|{T5, anspruchsvol...|  1302|   NULL|                NULL|               NULL|                 WS|                    NULL|                  NULL|          NULL|                NULL|[{Angelo & Ele, a...|       891|\n",
      "| Hikerata Valgannese|104351|            Poncione|            Poncione|    14449|2. Februar 2016 u...|2016-02-02 23:22:00|         66|[{45.924854, 8.82...|{Welt, Italien, L...|2016-01-30|[{https://s.hikr....|   {T2, Bergwandern}|  1887|   1887|Timedelta: 11 day...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{cappef, cappef}...|       928|\n",
      "|Bazardüzü / Bazar...|100077|             Sputnik|             Sputnik|       31|9. Oktober 2015 u...|2015-10-09 20:31:00|        122|[{41.22078, 47.85...|{Welt, Azerbaijan...|2015-09-12|[{https://s.hikr....|{T3, anspruchsvol...|  1700|   1700|Timedelta: 2 days...|               NULL|                  L|                    NULL|                  NULL|          NULL|                NULL|[{Sputnik, Sputni...|      1650|\n",
      "|Großglockner (3.7...|100095|            pika8x14|            pika8x14|     8481|9. Oktober 2015 u...|2015-10-09 05:29:00|         80|[{47.02142, 12.68...|{Welt, Österreich...|2015-09-28|[{https://s.hikr....|                NULL|  1900|   1900|Timedelta: 2 days...|               NULL|                WS+|                    NULL|                  NULL|          NULL|                NULL|[{pika8x14, pika8...|      2185|\n",
      "|Hikerata (con man...|102747|            Poncione|            Poncione|    14449|14. Dezember 2015...|2015-12-14 22:42:00|        121|[{46.075157, 8.77...|{Welt, Italien, L...|2015-12-12|[{https://s.hikr....|   {T2, Bergwandern}|   470|    470|Timedelta: 4 days...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{Ewuska, Evuska}...|      1105|\n",
      "|Winterbaer`s Natu...|109715|          Winterbaer|          Winterbaer|     5927|21. Juli 2016 um ...|2016-07-21 00:56:00|         97|[{47.610126, 11.0...|{Welt, Deutschlan...|2016-07-17|[{https://s.hikr....|                NULL|  4053|   4053|Timedelta: 15 day...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{Winterbaer, Win...|      1074|\n",
      "|Tutti quanti in c...|102227|               Menek|               Menek|    17406|24. November 2015...|2015-11-24 17:13:00|         97|[{45.876534, 9.84...|{Welt, Italien, L...|2015-11-22|[{https://s.hikr....|{T3, anspruchsvol...|  1112|   1136|Timedelta: 6 days...|               NULL|                  L|                    NULL|                  NULL|          NULL|                NULL|[{grandemago, gra...|       606|\n",
      "|La Banda dei Matt...|100255|giorgio59m (Girov...|          giorgio59m|     3344|6. Oktober 2015 u...|2015-10-06 11:22:00|         58|[{46.518715, 8.61...|{Welt, Schweiz, T...|2015-10-04|[{https://s.hikr....|   {T2, Bergwandern}|   800|   NULL|Timedelta: 6 days...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{giorgio59m (Gir...|       750|\n",
      "|Motto Rotondo Q22...|102812|giorgio59m (Girov...|          giorgio59m|     3344|15. Dezember 2015...|2015-12-15 08:22:00|         97|[{46.16998, 9.260...|{Welt, Italien, L...|2015-12-13|[{https://s.hikr....| {T4-, Alpinwandern}|  1350|   NULL|                NULL|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{giorgio59m (Gir...|      1010|\n",
      "|Pizzata a Como......|104142|             Massimo|             Massimo|     5121|28. Januar 2016 u...|2016-01-28 22:11:00|         99|[{45.97573, 9.249...|{Welt, Italien, L...|2016-01-23|[{https://s.hikr....|{T3, anspruchsvol...|  2600|   2605|Timedelta: 13 day...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{Massimo, Massim...|      1424|\n",
      "|Rifugio Cortin - ...|104358|giorgio59m (Girov...|          giorgio59m|     3344|2. Februar 2016 u...|2016-02-02 20:30:00|         60|[{46.20226, 9.168...|{Welt, Schweiz, G...|2016-01-31|[{https://s.hikr....|{T3, anspruchsvol...|  1250|   NULL|Timedelta: 5 days...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{giorgio59m (Gir...|       865|\n",
      "|Doppio Compleanno...|107709|giorgio59m (Girov...|          giorgio59m|     3344|24. Mai 2016 um 1...|2016-05-24 15:07:00|         94|[{46.368313, 8.91...|{Welt, Schweiz, T...|2016-05-22|[{https://s.hikr....|   {T2, Bergwandern}|  1160|   NULL|Timedelta: 5 days...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{giorgio59m (Gir...|       495|\n",
      "+--------------------+------+--------------------+--------------------+---------+--------------------+-------------------+-----------+--------------------+--------------------+----------+--------------------+--------------------+------+-------+--------------------+-------------------+-------------------+------------------------+----------------------+--------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsedTours_df = tours_df.select(parse_udf(col(\"value\"), col(\"file_path\")).alias(\"parsed_data\")).select(\"parsed_data.*\")\n",
    "\n",
    "parsedTours_df.cache()\n",
    "\n",
    "# Show the parsed DataFrame\n",
    "parsedTours_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8176"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedTours_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+---------+--------------------+-------------------+-----------+--------------------+--------------------+----------+--------------------+--------------------+------+-------+--------------------+-------------------+-------------------+------------------------+----------------------+--------------+--------------------+--------------------+----------+\n",
      "|                name|    id|  author_public_name|author_internal_name|author_id| publishing_date_str|    publishing_date|photo_count|               peaks|             regions| tour_date|           waypoints|   hiking_difficulty|ascent|descent|            duration|climbing_difficulty|hightour_difficulty|mountain_bike_difficulty|via_ferrata_difficulty|ski_difficulty| snowshoe_difficulty|        tour_partner|page_views|\n",
      "+--------------------+------+--------------------+--------------------+---------+--------------------+-------------------+-----------+--------------------+--------------------+----------+--------------------+--------------------+------+-------+--------------------+-------------------+-------------------+------------------------+----------------------+--------------+--------------------+--------------------+----------+\n",
      "|Bänke und ihre Au...|108442|          Vielhygler|          vielhygler|    20481|8. November 2016 ...|2016-11-08 23:36:00|        287|[{47.513954, 11.5...|{Welt, Überall, N...|2018-01-20|[{https://s.hikr....|                NULL|  NULL|   NULL|                NULL|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{Winterbaer, Win...|      3614|\n",
      "|  Wandern mit Katzen|100123|                mong|                mong|    12588|2. Januar 2016 um...|2016-01-02 23:32:00|        229|[{46.262966, 9.00...|{Welt, Schweiz, T...|2011-10-18|[{https://s.hikr....| {T4-, Alpinwandern}|   685|    658|Timedelta: 6 days...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|      [{mong, mong}]|      3190|\n",
      "|E5 und mehr - Von...|103600|        Nik Brückner|          Nikbrueckn|    13125|8. Februar 2016 u...|2016-02-08 18:54:00|        139|[{47.503807, 9.73...|{Welt, Terra Inco...|2009-08-07|[{https://s.hikr....| {T4+, Alpinwandern}| 18300|  18450|Timedelta: 17 day...|               NULL|                  L|                    NULL|                     L|          NULL|                NULL|[{Nik Brückner, N...|      3410|\n",
      "|Rifugio Alpe Arbe...|106073|giorgio59m (Girov...|          giorgio59m|     3344|31. März 2016 um ...|2016-03-31 09:07:00|         81|[{46.42771, 9.213...|{Welt, Schweiz, G...|2016-03-26|[{https://s.hikr....|                NULL|   970|   NULL|Timedelta: 5 days...|               NULL|               NULL|                    NULL|                  NULL|          NULL|{WT2, Schneeschuh...|[{giorgio59m (Gir...|      1088|\n",
      "|Garmisch - Vaduz:...|103599|        Nik Brückner|          Nikbrueckn|    13125|23. Januar 2016 u...|2016-01-23 12:09:00|        145|[{47.468254, 11.0...|{Welt, Liechtenst...|2011-08-18|[{https://s.hikr....|  {T4, Alpinwandern}| 19580|  19883|Timedelta: 17 day...|               NULL|                  L|                    NULL|                    WS|          NULL|                NULL|[{Nik Brückner, N...|      1531|\n",
      "|Rund um die Stadt...|106708|          laurentbor|          laurentbor|    23322|27. April 2016 um...|2016-04-27 11:33:00|        255|[{47.40749, 8.587...|{Welt, Schweiz, Z...|2013-05-01|[{https://s.hikr....|                NULL|  1159|   1159|Timedelta: 8 days...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{laurentbor, lau...|      1153|\n",
      "|       Upper Mustang|107904|            amphibol|               reeff|     7930|1. August 2016 um...|2016-08-01 20:42:00|        121|[{28.781376, 83.7...|{Welt, Nepal, Kal...|2016-05-10|[{https://s.hikr....|{T3+, anspruchsvo...|  5050|   4840|Timedelta: 12 day...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{amphibol, reeff...|      1085|\n",
      "|Se \"Angelino\" è G...|106488|               Menek|               Menek|    17406|14. April 2016 um...|2016-04-14 19:20:00|         47|[{45.932934, 8.98...|{Welt, Schweiz, T...|2016-04-12|[{https://s.hikr....|{T3, anspruchsvol...|  1360|   1360|                NULL|               NULL|               NULL|                    NULL|                    WS|          NULL|                NULL|[{gbal, gbal}, {M...|      1117|\n",
      "|CORVEGIA - SASSO ...|108828|        Angelo & Ele|          angeloeleo|    21082|29. Juni 2016 um ...|2016-06-29 19:26:00|         80|[{46.19195, 9.363...|{Welt, Italien, L...|2016-06-28|[{https://s.hikr....|{T5, anspruchsvol...|  1302|   NULL|                NULL|               NULL|                 WS|                    NULL|                  NULL|          NULL|                NULL|[{Angelo & Ele, a...|       891|\n",
      "| Hikerata Valgannese|104351|            Poncione|            Poncione|    14449|2. Februar 2016 u...|2016-02-02 23:22:00|         66|[{45.924854, 8.82...|{Welt, Italien, L...|2016-01-30|[{https://s.hikr....|   {T2, Bergwandern}|  1887|   1887|Timedelta: 11 day...|               NULL|               NULL|                    NULL|                  NULL|          NULL|                NULL|[{cappef, cappef}...|       928|\n",
      "+--------------------+------+--------------------+--------------------+---------+--------------------+-------------------+-----------+--------------------+--------------------+----------+--------------------+--------------------+------+-------+--------------------+-------------------+-------------------+------------------------+----------------------+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==================================================>(8027 + 12) / 8176]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----------+----------+\n",
      "|           peak_name|peak_id|peak_height|tour_count|\n",
      "+--------------------+-------+-----------+----------+\n",
      "|Rifugio Antoniett...|  19337|       1365|        50|\n",
      "|           Alpe Cova|  32366|       1311|        48|\n",
      "|         Alpe Devero|  10282|       1631|        46|\n",
      "|     Alpe del Vicerè|  23035|        900|        46|\n",
      "|          Wasserauen|   6854|        868|        46|\n",
      "|  Bocchetta di Lemna|  24364|       1167|        45|\n",
      "|            Wildhaus|   6853|       1090|        43|\n",
      "|Bocchetta di Palanzo|  24365|       1210|        42|\n",
      "|Cappella Sacro Cuore|  28975|        828|        42|\n",
      "|    Rifugio Brioschi|  21151|       2410|        40|\n",
      "| Bivacco Riva-Girani|  27701|       1862|        38|\n",
      "|      Rifugio Azzoni|   6616|       1860|        37|\n",
      "|        San Bernardo|  35186|       1620|        35|\n",
      "| Rifugio Alp de Volt|  36471|       1340|        35|\n",
      "|       Passo Forcora|  20149|       1190|        35|\n",
      "|     Monte Bolettone|  17092|       1320|        34|\n",
      "|        Capanna Mara|  17097|       1125|        34|\n",
      "|Monti di Breglia ...|  36259|       1089|        34|\n",
      "|          Monte Lema|   2886|       1621|        33|\n",
      "|            Resegone|   6617|       1875|        32|\n",
      "+--------------------+-------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parsedTours_df.createOrReplaceTempView(\"parsedTours\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"Select * from parsedTours limit 10\").show()\n",
    "# Create Ranking for the Peaks with the most tours\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        exploded_peaks.col.name AS peak_name,\n",
    "        exploded_peaks.col.id AS peak_id, \n",
    "        exploded_peaks.col.height AS peak_height ,   \n",
    "        COUNT(*) AS tour_count\n",
    "    FROM parsedTours\n",
    "    LATERAL VIEW explode(peaks) exploded_peaks\n",
    "    LATERAL VIEW explode(tour_partner) exploded_partners\n",
    "    GROUP BY \n",
    "          exploded_peaks.col\n",
    "    ORDER BY tour_count DESC, peak_height DESC\n",
    "    \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:==================================================>(8160 + 10) / 8176]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+------------------+----------+----------+------------------+\n",
      "|country        |total_tours|avg_ascent        |min_ascent|max_ascent|avg_photo_count   |\n",
      "+---------------+-----------+------------------+----------+----------+------------------+\n",
      "|Schweiz        |3882       |1042.3350815850815|5         |7630      |24.113858835651726|\n",
      "|Italien        |1837       |1109.2795484727756|30        |9900      |27.470332063146433|\n",
      "|Österreich     |907        |1018.928927680798 |5         |4980      |21.177508269018745|\n",
      "|Deutschland    |761        |812.5855728429985 |25        |4053      |23.692509855453352|\n",
      "|Spanien        |137        |601.9017857142857 |20        |2000      |26.255474452554743|\n",
      "|Frankreich     |131        |942.0943396226415 |45        |5700      |17.27480916030534 |\n",
      "|United States  |64         |413.14814814814815|30        |1550      |18.53125          |\n",
      "|Norwegen       |52         |1186.3863636363637|200       |2200      |24.442307692307693|\n",
      "|Island         |31         |727.0689655172414 |100       |2100      |24.70967741935484 |\n",
      "|Namibia        |27         |222.5             |30        |580       |18.814814814814813|\n",
      "|Liechtenstein  |23         |1612.4545454545455|45        |19580     |29.217391304347824|\n",
      "|Tschechien     |21         |410.75            |100       |800       |66.23809523809524 |\n",
      "|Terra Incognita|20         |2518.6923076923076|180       |18300     |30.05             |\n",
      "|Irland         |19         |769.5555555555555 |50        |2300      |21.842105263157894|\n",
      "|United Kindom  |19         |638.8823529411765 |150       |1000      |12.947368421052632|\n",
      "|Bolivien       |19         |933.3333333333334 |150       |5000      |16.0              |\n",
      "|Slowenien      |13         |1105.6363636363637|150       |1950      |26.692307692307693|\n",
      "|Ungarn         |12         |192.22222222222223|50        |450       |13.0              |\n",
      "|Peru           |11         |1243.0            |1200      |1286      |66.0909090909091  |\n",
      "|Romania        |11         |1045.4            |50        |2200      |24.90909090909091 |\n",
      "+---------------+-----------+------------------+----------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, min, max, count\n",
    "\n",
    "# Assuming you have a DataFrame named 'parsedTours' with the parsed tour data\n",
    "\n",
    "aggregated_data = parsedTours_df.groupBy(\"regions.country\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_tours\"),\n",
    "        avg(\"ascent\").alias(\"avg_ascent\"),\n",
    "        min(\"ascent\").alias(\"min_ascent\"),\n",
    "        max(\"ascent\").alias(\"max_ascent\"),\n",
    "        avg(\"photo_count\").alias(\"avg_photo_count\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_tours\").desc())\n",
    "\n",
    "aggregated_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DAWR Assignment 3 Spark Skeleton",
   "notebookOrigID": 3995684355508196,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
